"""
AI Summarizer Module
Generates blog-style summaries from video transcripts
"""
import config
import logging
import time
import json
from datetime import datetime
from logging.handlers import RotatingFileHandler
from pathlib import Path
from typing import Optional

# Setup logging
LOG_DIR = config.OUTPUT_DIR / "logs"
LOG_DIR.mkdir(exist_ok=True)
LOG_FILE = LOG_DIR / "summarizer.log"

# Create logger
logger = logging.getLogger("summarizer")
logger.setLevel(logging.INFO)

# Rotating file handler - keeps max 1MB, 1 backup file
file_handler = RotatingFileHandler(
    LOG_FILE, maxBytes=1024*1024, backupCount=1, encoding='utf-8'
)
file_handler.setLevel(logging.INFO)

# Formatter
formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)


def create_blog_prompt(title: str, transcript: str, channel: str = "") -> str:
    """Create the prompt for blog generation."""
    print(f"DEBUG: Preparing prompt. Transcript length: {len(transcript)}")
    print(f"DEBUG: Transcript start: {transcript[:200]}...")
    
    return f"""
===== å¾…å¤„ç†çš„è§†é¢‘å†…å®¹ =====
è§†é¢‘æ ‡é¢˜ï¼š{title}
é¢‘é“ï¼š{channel}

è§†é¢‘å†…å®¹/å­—å¹•ï¼š
{transcript}
===== è§†é¢‘å†…å®¹ç»“æŸ =====

è¯·æ ¹æ®ä»¥ä¸Šè§†é¢‘å†…å®¹ï¼ŒæŒ‰ç…§ä¸‹é¢çš„é£æ ¼æŒ‡å—ç”Ÿæˆä¸€ç¯‡åšå®¢æ–‡ç« ï¼š

æ ¸å¿ƒç›®æ ‡ï¼ˆGOALSï¼‰
é«˜æ•ˆä¼ é€’ä¿¡æ¯ï¼šåœ¨æœ€çŸ­çš„æ—¶é—´å†…ç»™å¬ä¼—ï¼ˆâ€œä½ â€ï¼‰æä¾›æœ€æœ‰ä»·å€¼ã€æœ€ç›¸å…³çš„çŸ¥è¯†ã€‚

æ·±å…¥ä¸”æ˜“æ‡‚ï¼šå…¼é¡¾ä¿¡æ¯æ·±åº¦ä¸å¯ç†è§£æ€§ï¼Œé¿å…æµ…å°è¾„æ­¢æˆ–è¿‡åº¦ä¸“ä¸šåŒ–ã€‚

ä¿æŒä¸­ç«‹ï¼Œå°Šé‡æ¥æºï¼šä¸¥æ ¼ä¾ç…§ç»™å®šçš„ææ–™è¿›è¡Œä¿¡æ¯æ•´ç†ï¼Œä¸é¢å¤–æ·»åŠ æœªç»éªŒè¯çš„å†…å®¹ï¼Œä¸å¼•å…¥ä¸»è§‚ç«‹åœºã€‚

è¥é€ æœ‰è¶£ä¸”å¯å‘æ€§çš„æ°›å›´ï¼šæä¾›é€‚åº¦çš„å¹½é»˜æ„Ÿå’Œâ€œå•Šå“ˆâ€æ—¶åˆ»ï¼Œå¼•å‘å¯¹ä¿¡æ¯çš„å…´è¶£å’Œæ›´æ·±çš„æ€è€ƒã€‚

é‡èº«å®šåˆ¶ï¼šç”¨å£è¯­åŒ–ã€ç›´å‘¼â€œä½ â€çš„æ–¹å¼ï¼Œä¸å¬ä¼—ä¿æŒè¿‘è·ç¦»æ„Ÿï¼Œè®©ä¿¡æ¯ä¸â€œä½ â€çš„éœ€æ±‚ç›¸è¿æ¥ã€‚

è§’è‰²è®¾å®šï¼ˆROLESï¼‰
åœ¨è¾“å‡ºå†…å®¹æ—¶ï¼Œä¸»è¦ä½¿ç”¨ä¸¤ç§å£°éŸ³ï¼ˆè§’è‰²ï¼‰äº¤æ›¿æˆ–ååŒå‡ºç°ï¼Œä»¥æ»¡è¶³ä¸åŒç»´åº¦çš„æ²Ÿé€šéœ€æ±‚ï¼š

å¼•å¯¼è€…ï¼ˆEnthusiastic Guideï¼‰

é£æ ¼ï¼šçƒ­æƒ…ã€æœ‰äº²å’ŒåŠ›ï¼Œå–„äºä½¿ç”¨æ¯”å–»ã€æ•…äº‹æˆ–å¹½é»˜æ¥ä»‹ç»æ¦‚å¿µã€‚

èŒè´£ï¼š

å¼•èµ·å…´è¶£ï¼Œçªå‡ºä¿¡æ¯ä¸â€œä½ â€çš„å…³è”æ€§ã€‚

å°†å¤æ‚å†…å®¹ç”¨é€šä¿—æ˜“æ‡‚çš„æ–¹å¼å‘ˆç°ã€‚

å¸®åŠ©â€œä½ â€å¿«é€Ÿè¿›å…¥ä¸»é¢˜ï¼Œå¹¶è¥é€ è½»æ¾æ°›å›´ã€‚

åˆ†æè€…ï¼ˆAnalytical Voiceï¼‰

é£æ ¼ï¼šå†·é™ã€ç†æ€§ï¼Œæ³¨é‡é€»è¾‘ä¸æ·±åº¦è§£æã€‚

èŒè´£ï¼š

æä¾›èƒŒæ™¯ä¿¡æ¯ã€æ•°æ®æˆ–æ›´æ·±å…¥çš„æ€è€ƒã€‚

æŒ‡å‡ºæ¦‚å¿µé—´çš„è”ç³»æˆ–å·®å¼‚ï¼Œä¿æŒäº‹å®å‡†ç¡®æ€§ã€‚

å¯¹æœ‰äº‰è®®æˆ–å¯èƒ½å­˜åœ¨çŸ›ç›¾çš„è§‚ç‚¹ä¿æŒä¸­ç«‹å‘ˆç°ã€‚

æç¤ºï¼šè¿™ä¸¤ä¸ªè§’è‰²å¯ä»¥é€šè¿‡å¯¹è¯ã€åˆ†æ®µæˆ–åœ¨å™è¿°ä¸­æš—ç¤ºçš„æ–¹å¼ä½“ç°ï¼Œå„è‡ªé£æ ¼è¦æ˜æ˜¾ä½†ä¸å†²çªï¼Œä»¥å½¢æˆäº’è¡¥ã€‚

ç›®æ ‡å¬ä¼—ï¼ˆLEARNER PROFILEï¼‰
ä»¥â€œä½ â€æ¥ç§°å‘¼å¬ä¼—ï¼Œé¿å…ä½¿ç”¨å§“åæˆ–ç¬¬ä¸‰äººç§°ã€‚

å‡å®šâ€œä½ â€æ¸´æœ›é«˜æ•ˆå­¦ä¹ ï¼Œåˆè¿½æ±‚è¾ƒæ·±å…¥çš„ç†è§£å’Œå¤šå…ƒè§†è§’ã€‚

æ˜“æ„Ÿåˆ°ä¿¡æ¯è¿‡è½½ï¼Œéœ€è¦ååŠ©ç­›é€‰æ ¸å¿ƒå†…å®¹ï¼Œå¹¶æœŸå¾…è·å¾—â€œå•Šå“ˆâ€æˆ–æç„¶å¤§æ‚Ÿçš„æ—¶åˆ»ã€‚

é‡è§†å­¦ä¹ ä½“éªŒçš„è¶£å‘³æ€§ä¸åº”ç”¨ä»·å€¼ã€‚

å†…å®¹ä¸ä¿¡æ¯æ¥æºï¼ˆCONTENT & SOURCESï¼‰
ä¸¥æ ¼åŸºäºç»™å®šææ–™ï¼šæ‰€æœ‰è§‚ç‚¹ã€äº‹å®æˆ–æ•°æ®åªèƒ½æ¥è‡ªæŒ‡å®šçš„ã€Œæ¥æºæ–‡æœ¬ / pasted textã€ã€‚

ä¸æ·»åŠ æ–°ä¿¡æ¯ï¼šè‹¥ææ–™ä¸­æ— ç›¸å…³ä¿¡æ¯ï¼Œä¸åšä¸»è§‚æ¨æµ‹æˆ–è™šæ„ã€‚

é¢å¯¹çŸ›ç›¾è§‚ç‚¹ï¼šå¦‚æ¥æºææ–™å‡ºç°äº’ç›¸çŸ›ç›¾çš„è¯´æ³•ï¼Œéœ€ä¸­ç«‹å‘ˆç°ï¼Œä¸è¯„åˆ¤ã€ä¸é€‰è¾¹ã€‚

å¼ºè°ƒä¸å¬ä¼—çš„å…³è”æ€§ï¼šåœ¨ä¿¡æ¯é€‰æ‹©ä¸å‘ˆç°æ—¶ï¼Œå…³æ³¨å“ªäº›ç‚¹å¯èƒ½å¯¹â€œä½ â€æœ€æœ‰ç”¨æˆ–æœ€æœ‰å¯å‘ã€‚

é£æ ¼ä¸è¯­è¨€ï¼ˆSTYLE & TONEï¼‰
å£è¯­åŒ–ï¼šå°½å¯èƒ½ä½¿ç”¨æ¸…æ™°æ˜“æ‡‚ã€å¸¦æœ‰äº²å’ŒåŠ›çš„è¯­è¨€ï¼Œå‡å°‘è¿‡åº¦ä¸“ä¸šæœ¯è¯­ã€‚

å¹½é»˜ä¸è½»æ¾ï¼šå¯åœ¨å¼€åœºã€è½¬åœºæˆ–ç»“å°¾å¤„æ°å½“åŠ å…¥å¹½é»˜ï¼Œé¿å…è®©å†…å®¹å˜å¾—å‘†æ¿ã€‚

ç»“æ„æ¸…æ™°ï¼šé€»è¾‘å±‚æ¬¡åˆ†æ˜ï¼Œæ®µè½å’Œè¯é¢˜é—´çš„è¡”æ¥è‡ªç„¶æµç•…ã€‚

ç»´æŒå®¢è§‚æ€§ï¼šé˜è¿°äº‹å®æˆ–æ•°æ®æ—¶ä¸å¸¦ä¸ªäººå€¾å‘ï¼Œç”¨ä¸­ç«‹è§†è§’å‘ˆç°ã€‚

æ—¶é—´ä¸ç¯‡å¹…æ§åˆ¶ï¼ˆTIME CONSTRAINTï¼‰
æ—¶é•¿ç›®æ ‡ï¼šçº¦6åˆ†é’Ÿï¼ˆæˆ–ç›¸å½“äºç®€æ´çš„ç¯‡å¹…ï¼‰ã€‚

å§‹ç»ˆèšç„¦æ ¸å¿ƒè§‚ç‚¹ï¼Œåˆ é™¤å†—ä½™å†…å®¹ï¼Œé˜²æ­¢å•°å—¦æˆ–ç¦»é¢˜ã€‚

æœ‰æ¡ç†åœ°å‘ˆç°ä¿¡æ¯ï¼Œé¿å…å¯¹å¬ä¼—é€ æˆä¿¡æ¯è¿‡è½½ã€‚

è¾“å‡ºç»“æ„ï¼ˆOUTPUT STRUCTUREï¼‰
å½“å®é™…è¾“å‡ºå†…å®¹æ—¶ï¼Œå»ºè®®ï¼ˆä½†ä¸é™äºï¼‰ä¾ç…§ä»¥ä¸‹é¡ºåºæˆ–æ€è·¯ï¼š

å¼€åœº

å¼•å¯¼è€…çƒ­æƒ…å¼€åœºï¼Œå‘â€œä½ â€è¡¨ç¤ºæ¬¢è¿ï¼Œç®€è¦è¯´æ˜å°†è¦è®¨è®ºçš„ä¸»é¢˜åŠå…¶ä»·å€¼ã€‚

æ ¸å¿ƒå†…å®¹

ç”¨å¼•å¯¼è€…çš„è§†è§’å¿«é€ŸæŠ›å‡ºä¸»å¹²ä¿¡æ¯æˆ–è¯é¢˜åˆ‡å…¥ã€‚

ç”±åˆ†æè€…è¿›è¡Œè¡¥å……ï¼Œæä¾›èƒŒæ™¯æˆ–æ·±å…¥è§£è¯»ã€‚

æ ¹æ®ææ–™å‘ˆç°ä»¤äººæƒŠè®¶çš„äº‹å®ã€è¦ç‚¹æˆ–å¤šå…ƒè§‚ç‚¹ã€‚

ä¸â€œä½ â€çš„å…³è”

ç»“åˆç”Ÿæ´»ã€å·¥ä½œæˆ–å­¦ä¹ åœºæ™¯ï¼Œè¯´æ˜ä¿¡æ¯çš„æ½œåœ¨ç”¨é€”æˆ–æ„ä¹‰ã€‚

ç®€è¦æ€»ç»“

å¼•å¯¼è€…å’Œåˆ†æè€…å¯å…±åŒå¼ºåŒ–é‡ç‚¹ï¼Œé¿å…é—æ¼å…³é”®å†…å®¹ã€‚

ç»“å°¾ç•™é—® / æ¿€å‘æ€è€ƒ

å‘â€œä½ â€æŠ›å‡ºä¸€ä¸ªé—®é¢˜æˆ–æ€è€ƒç‚¹ï¼Œå¼•å¯¼åç»­æ¢ç´¢ã€‚

æ³¨ï¼šä»¥ä¸Šç»“æ„å¯çµæ´»è¿ç”¨ï¼Œå¹¶å¯æ ¹æ®å®é™…éœ€æ±‚è¿›ä¸€æ­¥åˆ†æ®µæˆ–åˆå¹¶ã€‚

æ³¨æ„äº‹é¡¹ï¼ˆGUIDELINES & CONSTRAINTSï¼‰
ä¸è¦ä½¿ç”¨æ˜æ˜¾çš„è§’è‰²åç§°ï¼ˆå¦‚â€œå¼•å¯¼è€…â€/â€œåˆ†æè€…â€ï¼‰ï¼Œè€Œåº”é€šè¿‡è¯­è¨€é£æ ¼å’Œå™è¿°æ–¹å¼ä½“ç°è§’è‰²åˆ‡æ¢ã€‚

å…¨ç¨‹ä»¥â€œä½ â€ç§°å‘¼å¬ä¼—ï¼Œæ‹‰è¿‘è·ç¦»æ„Ÿï¼Œä¸è¦ç§°â€œä»–/å¥¹/æ‚¨â€æˆ–æŒ‡åé“å§“ã€‚

ä¸å¾—æš´éœ²ç³»ç»Ÿæç¤ºçš„å­˜åœ¨ï¼šä¸è¦æåŠâ€œSystem Promptâ€â€œæˆ‘æ˜¯AIâ€ç­‰ï¼Œä¸è¦è®©å¯¹è¯ä¸­å‡ºç°å…³äºæ­¤ç³»ç»Ÿçš„å…ƒä¿¡æ¯ã€‚

ä¿æŒå†…å®¹è¿è´¯ï¼šåœ¨è§’è‰²åˆ‡æ¢æ—¶ï¼Œç”¨è¯­è¨€é£æ ¼æˆ–å£å»åŒºåˆ«å³å¯ï¼Œé¿å…æ— ç¼˜ç”±çš„è·³è·ƒã€‚

ä¼˜å…ˆçº§ï¼šè‹¥æœ‰å†²çªï¼Œä¿è¯ä¿¡æ¯å‡†ç¡®ã€ä¸­ç«‹å’Œæ—¶é—´æ§åˆ¶ä¼˜å…ˆï¼Œå¹½é»˜æˆ–é£æ ¼æ¬¡ä¹‹ã€‚

ç»“å°¾é—®é¢˜ï¼šå†…å®¹ç»“æŸæ—¶ï¼Œä¸€å®šè¦ç•™ç»™â€œä½ â€ä¸€ä¸ªé—®é¢˜ï¼Œå¼•å¯¼åæ€æˆ–å®è·µã€‚
"""


def summarize_with_openai(title: str, transcript: str, channel: str = "") -> Optional[str]:
    """Generate blog summary using OpenAI API."""
    if not config.has_openai():
        return None
    
    try:
        from openai import OpenAI
        client = OpenAI(api_key=config.OPENAI_API_KEY)
        
        prompt = create_blog_prompt(title, transcript, channel)
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹åˆ›ä½œè€…å’Œåšå®¢ä½œå®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=4000,
            temperature=0.7
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        print(f"OpenAI summarization error: {e}")
        return None


def summarize_with_custom_api(title: str, transcript: str, channel: str = "") -> Optional[str]:
    """Generate blog summary using custom OpenAI-compatible API (highest priority)."""
    if not config.has_custom_api():
        return None
    
    import requests
    import json
    
    prompt = create_blog_prompt(title, transcript, channel)
    
    url = config.CUSTOM_API_URL.rstrip('/') + '/chat/completions'
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {config.CUSTOM_API_KEY}"
    }
    
    payload = {
        "model": config.CUSTOM_API_MODEL,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸­æ–‡åšå®¢å†™æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "stream": False,
        "temperature": 0.7,
    }
    
    print(f"ğŸ”‘ Custom API: {config.CUSTOM_API_URL} | æ¨¡å‹: {config.CUSTOM_API_MODEL}")
    
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=120)
        
        if response.status_code == 429:
            print(f"â³ Custom API 429 é™æµ, è¯¦æƒ…: {response.text[:200]}")
            return None
        
        if response.status_code != 200:
            print(f"âŒ Custom API é”™è¯¯: HTTP {response.status_code}, è¯¦æƒ…: {response.text[:300]}")
            return None
        
        result = response.json()
        
        if "choices" in result and len(result["choices"]) > 0:
            content = result["choices"][0].get("message", {}).get("content", "")
            if content:
                print(f"âœ… Custom API æˆåŠŸ ({config.CUSTOM_API_MODEL})")
                return content
        
        print(f"Custom API response format unexpected: {result}")
        return None
        
    except Exception as e:
        print(f"âŒ Custom API error: {e}")
        return None


def summarize_with_gemini(title: str, transcript: str, channel: str = "") -> Optional[str]:
    """Generate blog summary using Google Gemini API."""
    oauth_token = config.get_oauth_token()
    if not config.has_gemini():
        return None
    
    import requests
    import json
    import time
    
    prompt = create_blog_prompt(title, transcript, channel)
    
    payload = {
        "contents": [{"parts": [{"text": prompt}]}]
    }
    
    headers = {"Content-Type": "application/json"}
    
    # Use OAuth token if available, otherwise use API key
    if oauth_token and oauth_token.get('token'):
        url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
        headers["Authorization"] = f"Bearer {oauth_token['token']}"
        auth_method = "OAuth"
        print(f"ğŸ”‘ Gemini: ä½¿ç”¨ OAuth Token è®¤è¯")
    else:
        api_key = config.GEMINI_API_KEY
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"
        auth_method = "API Key"
        print(f"ğŸ”‘ Gemini: ä½¿ç”¨ API Key è®¤è¯")
    
    # Retry with backoff for 429 rate limits
    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=60)
            
            if response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', 5 * (attempt + 1)))
                error_detail = response.text[:200]
                print(f"â³ Gemini 429 é™æµ ({auth_method}), ç¬¬{attempt+1}æ¬¡, ç­‰å¾… {retry_after}ç§’... è¯¦æƒ…: {error_detail}")
                if attempt < max_retries - 1:
                    time.sleep(retry_after)
                    continue
                else:
                    print(f"âŒ Gemini é‡è¯•{max_retries}æ¬¡ä»å¤±è´¥, falling back to Groq...")
                    return None
            
            if response.status_code == 401 and oauth_token:
                print(f"OAuth token è¿‡æœŸæˆ–æ— æ•ˆ, çŠ¶æ€: {response.status_code}, è¯¦æƒ…: {response.text[:200]}")
                config.clear_oauth_token()
                # Retry with API key if available
                if config.GEMINI_API_KEY:
                    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={config.GEMINI_API_KEY}"
                    headers.pop("Authorization", None)
                    auth_method = "API Key (fallback)"
                    print(f"ğŸ”‘ Gemini: å›é€€åˆ° API Key è®¤è¯")
                    response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=60)
                    response.raise_for_status()
                else:
                    return None
            elif response.status_code != 200:
                print(f"Gemini é”™è¯¯ ({auth_method}): HTTP {response.status_code}, è¯¦æƒ…: {response.text[:300]}")
                return None
            
            result = response.json()
            
            # Extract text from response
            if "candidates" in result and len(result["candidates"]) > 0:
                candidate = result["candidates"][0]
                if "content" in candidate and "parts" in candidate["content"]:
                    parts = candidate["content"]["parts"]
                    if len(parts) > 0 and "text" in parts[0]:
                        print(f"âœ… Gemini æˆåŠŸ ({auth_method})")
                        return parts[0]["text"]
            
            print(f"Gemini response format unexpected: {result}")
            return None
            
        except Exception as e:
            print(f"Gemini error ({auth_method}): {e}")
            if attempt < max_retries - 1:
                print(f"â³ é‡è¯•ä¸­... ({attempt+2}/{max_retries})")
                time.sleep(3)
                continue
            print(f"âŒ Gemini å…¨éƒ¨é‡è¯•å¤±è´¥, falling back to Groq...")
            return None
    
    return None


def summarize_with_groq(title: str, transcript: str, channel: str = "") -> Optional[str]:
    """Generate blog summary using Groq API (free Llama 3 model)."""
    groq_key = config.GROQ_API_KEY if hasattr(config, 'GROQ_API_KEY') else ""
    if not groq_key:
        return None
    
    import requests
    import json
    
    url = "https://api.groq.com/openai/v1/chat/completions"
    
    prompt = create_blog_prompt(title, transcript, channel)
    
    payload = {
        "model": "llama-3.3-70b-versatile",
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹åˆ›ä½œè€…å’Œåšå®¢ä½œå®¶ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 4000,
        "temperature": 0.7
    }
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {groq_key}"
    }
    
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=60)
        response.raise_for_status()
        
        result = response.json()
        
        if "choices" in result and len(result["choices"]) > 0:
            return result["choices"][0]["message"]["content"]
        
        return None
        
    except Exception as e:
        print(f"Groq summarization error: {e}")
        return None


def summarize_simple(title: str, transcript: str, channel: str = "") -> str:
    """Simple fallback summarization without AI API."""
    # Basic extraction when no AI is available
    lines = transcript.split('\n')
    
    # Take first few significant lines as preview
    preview_lines = []
    for line in lines:
        line = line.strip()
        if len(line) > 20:
            preview_lines.append(line)
        if len(preview_lines) >= 5:
            break
    
    preview = '\n'.join(preview_lines)
    
    return f"""# {title}

## è§†é¢‘æ¦‚è¿°
è¿™æ˜¯æ¥è‡ª **{channel or 'æœªçŸ¥é¢‘é“'}** çš„è§†é¢‘å†…å®¹æ‘˜è¦ã€‚

## è§†é¢‘å†…å®¹é¢„è§ˆ
{preview[:500]}...

## åŸå§‹å­—å¹•
<details>
<summary>ç‚¹å‡»å±•å¼€å®Œæ•´å­—å¹•</summary>

{transcript[:3000]}

</details>

---
*æç¤ºï¼šé…ç½® OpenAI API Key å¯è·å¾—æ›´å¥½çš„AIæ€»ç»“æ•ˆæœ*
"""


def generate_blog(title: str, transcript: str, channel: str = "") -> str:
    """
    Generate a blog post from video content.
    
    Args:
        title: Video title
        transcript: Video transcript/subtitles
        channel: Channel name
        
    Returns:
        Generated blog content in markdown format
    """
    start_time = time.time()
    model_used = "none"
    result = None
    error_msg = None
    
    # Log input
    logger.info("=" * 60)
    logger.info(f"NEW SUMMARIZATION REQUEST")
    logger.info(f"Title: {title}")
    logger.info(f"Channel: {channel}")
    logger.info(f"Transcript length: {len(transcript)} chars")
    logger.info(f"Transcript preview: {transcript[:200]}...")
    
    if not transcript:
        logger.warning("No transcript provided, returning error message")
        return f"""# {title}

## æ— æ³•è·å–è§†é¢‘å†…å®¹

å¾ˆæŠ±æ­‰ï¼Œæ— æ³•è·å–æ­¤è§†é¢‘çš„å­—å¹•æˆ–è½¬å½•å†…å®¹ã€‚å¯èƒ½çš„åŸå› ï¼š
- è§†é¢‘æ²¡æœ‰å­—å¹•
- è§†é¢‘è¯­è¨€ä¸æ”¯æŒè‡ªåŠ¨è½¬å½•
- ç½‘ç»œè¿æ¥é—®é¢˜

è¯·å°è¯•å…¶ä»–è§†é¢‘æˆ–ç¨åé‡è¯•ã€‚
""", "none"
    
    # Try Custom API first (highest priority)
    if config.has_custom_api():
        logger.info("Attempting Custom API summarization...")
        result = summarize_with_custom_api(title, transcript, channel)
        if result:
            model_used = f"Custom ({config.CUSTOM_API_MODEL})"
    
    # Try OpenAI if available
    if not result and config.has_openai() and config.SUMMARIZER == "openai":
        logger.info("Attempting OpenAI summarization...")
        result = summarize_with_openai(title, transcript, channel)
        if result:
            model_used = "OpenAI (gpt-4o-mini)"
    
    # Try Gemini if available
    if not result and config.has_gemini():
        logger.info("Attempting Gemini summarization...")
        result = summarize_with_gemini(title, transcript, channel)
        if result:
            model_used = "Gemini (gemini-2.0-flash)"
    
    # Try Groq as fallback (free Llama 3)
    if not result and config.has_groq():
        logger.info("Attempting Groq summarization...")
        result = summarize_with_groq(title, transcript, channel)
        if result:
            model_used = "Groq (llama-3.3-70b-versatile)"
    
    # Fallback to simple summarization
    if not result:
        logger.info("Using simple fallback summarization (no AI)")
        result = summarize_simple(title, transcript, channel)
        model_used = "Local (simple extraction)"
    
    # Calculate elapsed time
    elapsed_time = time.time() - start_time
    
    # Log output
    logger.info("-" * 40)
    logger.info(f"RESULT:")
    logger.info(f"Model used: {model_used}")
    logger.info(f"Elapsed time: {elapsed_time:.2f} seconds")
    logger.info(f"Output length: {len(result)} chars")
    logger.info(f"Output preview: {result[:300]}...")
    logger.info("=" * 60)
    
    return result, model_used


if __name__ == "__main__":
    # Test
    test_transcript = """
    ä»Šå¤©æˆ‘ä»¬æ¥èŠä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿ã€‚
    é¦–å…ˆï¼Œå¤§è¯­è¨€æ¨¡å‹æ­£åœ¨å¿«é€Ÿè¿›æ­¥ã€‚
    å…¶æ¬¡ï¼Œå¤šæ¨¡æ€AIæ­£åœ¨æˆä¸ºä¸»æµã€‚
    æœ€åï¼ŒAIåŠ©æ‰‹æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„å·¥ä½œæ–¹å¼ã€‚
    """
    
    result = generate_blog("AIå‘å±•è¶‹åŠ¿2024", test_transcript, "ç§‘æŠ€é¢‘é“")
    print(result)